{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy==1.6.0\n",
    "# !pip install matplotlib==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "random.seed(1)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, Image\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIF360\n",
    "import aif360\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.sklearn.datasets import fetch_compas\n",
    "# fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric, DatasetMetric\n",
    "from aif360.metrics.common_utils import compute_metrics\n",
    "from aif360.metrics.utils import compute_num_instances\n",
    "# data preprocessing\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n",
    "# explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "# bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover, GerryFairClassifier\n",
    "from aif360.sklearn.inprocessing import AdversarialDebiasing as SKLearnAdversarialDebiasing\n",
    "from aif360.algorithms.inprocessing.gerryfair.clean import array_to_tuple\n",
    "from aif360.algorithms.inprocessing.gerryfair.auditor import Auditor\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, RejectOptionClassification\n",
    "from aif360.sklearn.utils import check_inputs, check_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# scalers\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "# classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree, linear_model\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, roc_curve, classification_report, confusion_matrix\n",
    "# kernels\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = '/Users/megantennies/FYP/saved data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(filename, data):\n",
    "    with open(os.path.join(FILES, filename), 'w') as write:\n",
    "        json.dump(data, write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(filename):\n",
    "    with open(filename, 'r') as load:\n",
    "        data = json.load(load)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(filename, df):\n",
    "    df.to_json(os.path.join(FILES, filename), orient = 'split', \n",
    "    compression = 'infer', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_graphs(metric_name, filename):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'race': 1}]\n",
    "unprivileged_groups = [{'race': 0}]\n",
    "original_dataset = load_preproc_data_compas(['race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mappings = {'label_maps': [{1.0: 'Recid', 0.0: 'Non-Recid'}], \n",
    "    'protected_attribute_maps': [{1.0: 'Male', 0.0: 'Female'}, \n",
    "    {1.0: 'White', 0.0: 'Non-White'}]}\n",
    "metrics = ['Statistical parity difference', 'Average odds difference', \n",
    "    'Equal opportunity difference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train, original_val_test = original_dataset.split([0.7], shuffle = True)\n",
    "original_val, original_test = original_val_test.split([0.5], shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(train = None, val = None, test = None):\n",
    "    if train is not None:\n",
    "        display(Markdown('#### Training dataset shape'))\n",
    "        print(train.features.shape)\n",
    "    if val is not None:\n",
    "        display(Markdown('#### Validation dataset shape'))\n",
    "        print(val.features.shape)\n",
    "    display(Markdown('#### Test dataset shape'))\n",
    "    print(test.features.shape)\n",
    "    display(Markdown('#### Favorable and unfavorable labels'))\n",
    "    print(test.favorable_label, test.unfavorable_label)\n",
    "    display(Markdown('#### Protected attribute names'))\n",
    "    print(test.protected_attribute_names)\n",
    "    display(Markdown('#### Privileged and unprivileged protected attribute values'))\n",
    "    print(test.privileged_protected_attributes, test.unprivileged_protected_attributes)\n",
    "    display(Markdown(\"#### Dataset feature names\"))\n",
    "    print(train.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3694, 10)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Validation dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(792, 10)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Test dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(792, 10)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'race', 'age_cat=25 to 45', 'age_cat=Greater than 45', 'age_cat=Less than 25', 'priors_count=0', 'priors_count=1 to 3', 'priors_count=More than 3', 'c_charge_degree=F', 'c_charge_degree=M']\n"
     ]
    }
   ],
   "source": [
    "describe(original_train, original_val, original_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_metric = BinaryLabelDatasetMetric(original_train, \n",
    "    unprivileged_groups = unprivileged_groups, \n",
    "    privileged_groups = privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original COMPAS training data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: Difference in mean outcomes between unprivileged and privileged groups = -0.130993\n",
      "Validation data: Difference in mean outcomes between unprivileged and privileged groups = -0.101246\n",
      "Testing data: Difference in mean outcomes between unprivileged and privileged groups = -0.175808\n"
     ]
    }
   ],
   "source": [
    "original_explainer = MetricTextExplainer(original_metric)\n",
    "display(Markdown('#### Original COMPAS training data'))\n",
    "\n",
    "original_train_metric = BinaryLabelDatasetMetric(original_train, \n",
    "    unprivileged_groups = unprivileged_groups, privileged_groups = privileged_groups)\n",
    "print('Training data: Difference in mean outcomes between unprivileged and privileged groups = %f' % original_train_metric.mean_difference())\n",
    "\n",
    "original_val_metric = BinaryLabelDatasetMetric(original_val, \n",
    "    unprivileged_groups = unprivileged_groups, privileged_groups = privileged_groups)\n",
    "print('Validation data: Difference in mean outcomes between unprivileged and privileged groups = %f' % original_val_metric.mean_difference())\n",
    "\n",
    "original_test_metric = BinaryLabelDatasetMetric(original_test, \n",
    "    unprivileged_groups = unprivileged_groups, privileged_groups = privileged_groups)\n",
    "print('Testing data: Difference in mean outcomes between unprivileged and privileged groups = %f' % original_test_metric.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scaler = StandardScaler()\n",
    "X_train = original_scaler.fit_transform(original_train.features)\n",
    "y_train = original_train.labels.ravel()\n",
    "w_train = original_train.instance_weights.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train, sample_weight = original_train.instance_weights)\n",
    "y_train_preds = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ind = np.where(lr.classes_ == original_train.favorable_label)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_preds = original_train.copy()\n",
    "original_train_preds.labels = y_train_preds\n",
    "\n",
    "original_val_preds = original_val.copy(deepcopy = True)\n",
    "X_val = original_scaler.transform(original_val_preds.features)\n",
    "y_val = original_val_preds.labels\n",
    "original_val_preds.scores = lr.predict_proba(X_val)[:, pos_ind].reshape(-1, 1)\n",
    "\n",
    "original_test_preds = original_test.copy(deepcopy = True)\n",
    "X_test = original_scaler.transform(original_test_preds.features)\n",
    "y_test = original_test_preds.labels\n",
    "original_test_preds.scores = lr.predict_proba(X_test)[:, pos_ind].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_thresh = 100\n",
    "bal_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    fav_inds = original_val_preds.scores > class_thresh\n",
    "    original_val_preds.labels[fav_inds] = original_val_preds.favorable_label\n",
    "    original_val_preds.labels[~fav_inds] = original_val_preds.unfavorable_label\n",
    "\n",
    "    original_val_metric = ClassificationMetric(original_val, \n",
    "        original_val_preds, unprivileged_groups = unprivileged_groups, \n",
    "        privileged_groups = privileged_groups)\n",
    "\n",
    "    bal_arr[idx] = 0.5 * (original_val_metric.true_positive_rate() \n",
    "        + original_val_metric.true_negative_rate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original COMPAS data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no transforming): 0.6685\n",
      "Optimal classification threshold (no transforming): 0.4357\n"
     ]
    }
   ],
   "source": [
    "best_ind = np.where(bal_arr == np.max(bal_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "display(Markdown('#### Original COMPAS data'))\n",
    "print('Best balanced accuracy (no transforming): %.4f' % np.max(bal_arr))\n",
    "print('Optimal classification threshold (no transforming): %.4f' % best_class_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_acc_arr = []\n",
    "disp_imp_arr = []\n",
    "avg_odds_diff_arr = []\n",
    "eq_opp_diff_arr = []\n",
    "outcome_unfair_arr = []\n",
    "acc_equal_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Predictions from the original testing data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:00<00:00, 314.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification threshold used: 0.4357\n",
      "Balanced accuracy = 0.6645\n",
      "Statistical parity difference = -0.2571\n",
      "Disparate impact = 0.6648\n",
      "Average odds difference = -0.2272\n",
      "Equal opportunity difference = -0.1254\n",
      "Theil index = 0.2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [00:00<00:00, 205.90it/s]invalid value encountered in double_scalars\n",
      "100%|██████████| 100/100 [00:00<00:00, 249.43it/s]\n"
     ]
    }
   ],
   "source": [
    "display(Markdown('#### Predictions from the original testing data'))\n",
    "print('Classification threshold used: %.4f' % best_class_thresh)\n",
    "\n",
    "for thresh in tqdm(class_thresh_arr):\n",
    "    if thresh == best_class_thresh:\n",
    "        disp = True\n",
    "    else:\n",
    "        disp = False\n",
    "    fav_inds = original_test_preds.scores > thresh\n",
    "    original_test_preds.labels[fav_inds] = original_test_preds.favorable_label\n",
    "    original_test_preds.labels[~fav_inds] = original_test_preds.unfavorable_label\n",
    "    \n",
    "    metric_test = compute_metrics(original_test, original_test_preds, \\\n",
    "        unprivileged_groups, privileged_groups, disp = disp)\n",
    "    class_metric_test = ClassificationMetric(original_test, original_test_preds, \\\n",
    "        unprivileged_groups, privileged_groups)\n",
    "    \n",
    "    bal_acc_arr.append(metric_test['Balanced accuracy'])\n",
    "    avg_odds_diff_arr.append(metric_test['Average odds difference'])\n",
    "    disp_imp_arr.append(metric_test['Disparate impact'])\n",
    "    eq_opp_diff_arr.append(metric_test['Equal opportunity difference'])\n",
    "    outcome_unfair_arr.append(class_metric_test.false_discovery_rate_difference() \\\n",
    "        + class_metric_test.false_positive_rate_difference())\n",
    "    acc_equal_arr.append((class_metric_test.true_positive_rate(privileged = False) + \\\n",
    "        class_metric_test.true_negative_rate(privileged = False)) - \\\n",
    "            (class_metric_test.true_positive_rate(privileged = True) + \\\n",
    "                class_metric_test.true_negative_rate(privileged = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_bal_acc = np.interp(best_class_thresh, class_thresh_arr, bal_acc_arr)\n",
    "LR_acc_equal = np.interp(best_class_thresh, class_thresh_arr, acc_equal_arr)\n",
    "LR_disp_imp = np.interp(best_class_thresh, class_thresh_arr, disp_imp_arr)\n",
    "LR_out_unf = np.interp(best_class_thresh, class_thresh_arr, outcome_unfair_arr)\n",
    "LR_avg_odds = np.interp(best_class_thresh, class_thresh_arr, avg_odds_diff_arr)\n",
    "LR_eq_odds = np.interp(best_class_thresh, class_thresh_arr, eq_opp_diff_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Logistic Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.664457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy Equality</td>\n",
       "      <td>0.203694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disparate Impact</td>\n",
       "      <td>0.664821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Outcome Unfairess</td>\n",
       "      <td>-0.285388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Average Odds Difference</td>\n",
       "      <td>-0.227238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Equal Opportunity Difference</td>\n",
       "      <td>-0.125391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Metric  Logistic Regression\n",
       "0             Balanced Accuracy             0.664457\n",
       "1             Accuracy Equality             0.203694\n",
       "2              Disparate Impact             0.664821\n",
       "3             Outcome Unfairess            -0.285388\n",
       "4       Average Odds Difference            -0.227238\n",
       "5  Equal Opportunity Difference            -0.125391"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_results = {'Metric': ['Balanced Accuracy', 'Accuracy Equality', 'Disparate Impact', 'Outcome Unfairess', 'Average Odds Difference', 'Equal Opportunity Difference'],\n",
    "    'Logistic Regression': [LR_bal_acc, LR_acc_equal, LR_disp_imp, LR_out_unf, LR_avg_odds, LR_eq_odds]}\n",
    "LR_results_df = pd.DataFrame(LR_results)\n",
    "LR_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_json(filename = 'LR_results.json', df = LR_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = Reweighing(unprivileged_groups = unprivileged_groups, \n",
    "    privileged_groups = privileged_groups)\n",
    "rw.fit(original_train)\n",
    "reweighed_train = rw.transform(original_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(reweighed_train.instance_weights.sum() \n",
    "    - original_train.instance_weights.sum()) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Reweighed COMPAS training data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.9999999999999998\n",
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.130993\n"
     ]
    }
   ],
   "source": [
    "reweighed_metric = BinaryLabelDatasetMetric(reweighed_train, \n",
    "    unprivileged_groups = unprivileged_groups, privileged_groups = privileged_groups)\n",
    "\n",
    "reweighed_explainer = MetricTextExplainer(reweighed_metric)\n",
    "display(Markdown('#### Reweighed COMPAS training data'))\n",
    "print(reweighed_explainer.disparate_impact())\n",
    "\n",
    "reweighed_train_metric = BinaryLabelDatasetMetric(reweighed_train, \n",
    "    unprivileged_groups = unprivileged_groups, privileged_groups = privileged_groups)\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % original_train_metric.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(reweighed_metric.mean_difference()) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweighed_scaler = StandardScaler()\n",
    "X_train_rw = reweighed_scaler.fit_transform(reweighed_train.features)\n",
    "y_train_rw = reweighed_train.labels.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_rw, y_train_rw, sample_weight = reweighed_train.instance_weights)\n",
    "y_train_preds_rw = lr.predict(X_train_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweighed_test_preds = original_test.copy(deepcopy = True)\n",
    "X_test_rw = reweighed_scaler.fit_transform(reweighed_test_preds.features)\n",
    "y_test_rw = reweighed_test_preds.labels\n",
    "reweighed_test_preds.scores = lr.predict_proba(X_test_rw)[:, pos_ind].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW_bal_acc_arr = []\n",
    "RW_disp_imp_arr = []\n",
    "RW_avg_odds_diff_arr = []\n",
    "RW_eq_opp_diff_arr = []\n",
    "RW_outcome_unfair_arr = []\n",
    "RW_acc_equal_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Predictions from the transformed testing data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [00:00<00:00, 339.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification threshold used: 0.4357\n",
      "Balanced accuracy = 0.6385\n",
      "Statistical parity difference = -0.1195\n",
      "Disparate impact = 0.8306\n",
      "Average odds difference = -0.0975\n",
      "Equal opportunity difference = 0.0166\n",
      "Theil index = 0.2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "divide by zero encountered in double_scalars\n",
      "invalid value encountered in double_scalars\n",
      "100%|██████████| 100/100 [00:00<00:00, 364.44it/s]\n"
     ]
    }
   ],
   "source": [
    "display(Markdown('#### Predictions from the transformed testing data'))\n",
    "print('Classification threshold used: %.4f' % best_class_thresh)\n",
    "\n",
    "for thresh in tqdm(class_thresh_arr):\n",
    "    if thresh == best_class_thresh:\n",
    "        disp = True\n",
    "    else:\n",
    "        disp = False\n",
    "    fav_inds = reweighed_test_preds.scores > thresh\n",
    "    reweighed_test_preds.labels[fav_inds] = reweighed_test_preds.favorable_label\n",
    "    reweighed_test_preds.labels[~fav_inds] = reweighed_test_preds.unfavorable_label\n",
    "    \n",
    "    rw_metric_test = compute_metrics(original_test, reweighed_test_preds, \n",
    "        unprivileged_groups, privileged_groups, disp = disp)\n",
    "    rw_class_metric_test = ClassificationMetric(original_test, reweighed_test_preds, \\\n",
    "        unprivileged_groups, privileged_groups)\n",
    "    \n",
    "    RW_bal_acc_arr.append(rw_metric_test['Balanced accuracy'])\n",
    "    RW_avg_odds_diff_arr.append(rw_metric_test['Average odds difference'])\n",
    "    RW_disp_imp_arr.append(rw_metric_test['Disparate impact'])\n",
    "    RW_eq_opp_diff_arr.append(rw_metric_test['Equal opportunity difference'])\n",
    "    RW_outcome_unfair_arr.append(rw_class_metric_test.false_discovery_rate_difference() \\\n",
    "        + rw_class_metric_test.false_positive_rate_difference())\n",
    "    RW_acc_equal_arr.append((rw_class_metric_test.true_positive_rate(privileged = False) + \\\n",
    "        rw_class_metric_test.true_negative_rate(privileged = False)) - \\\n",
    "            (rw_class_metric_test.true_positive_rate(privileged = True) + \\\n",
    "                rw_class_metric_test.true_negative_rate(privileged = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(filename = 'RW_bal_acc_arr.json', data = RW_bal_acc_arr)\n",
    "save_to_json(filename = 'RW_disp_imp_arr.json', data = RW_disp_imp_arr)\n",
    "save_to_json(filename = 'RW_avg_odds_diff_arr.json', data = RW_avg_odds_diff_arr)\n",
    "save_to_json(filename = 'RW_eq_opp_diff_arr.json', data = RW_eq_opp_diff_arr)\n",
    "save_to_json(filename = 'RW_outcome_unfair_arr.json', data = RW_outcome_unfair_arr)\n",
    "save_to_json(filename = 'RW_acc_equal_arr.json', data = RW_acc_equal_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW_bal_acc = np.interp(best_class_thresh, class_thresh_arr, RW_bal_acc_arr)\n",
    "RW_acc_equal = np.interp(best_class_thresh, class_thresh_arr, RW_acc_equal_arr)\n",
    "RW_disp_imp = np.interp(best_class_thresh, class_thresh_arr, RW_disp_imp_arr)\n",
    "RW_out_unf = np.interp(best_class_thresh, class_thresh_arr, RW_outcome_unfair_arr)\n",
    "RW_avg_odds = np.interp(best_class_thresh, class_thresh_arr, RW_avg_odds_diff_arr)\n",
    "RW_eq_odds = np.interp(best_class_thresh, class_thresh_arr, RW_eq_opp_diff_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Reweighing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.638525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy Equality</td>\n",
       "      <td>0.228116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disparate Impact</td>\n",
       "      <td>0.830641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Outcome Unfairess</td>\n",
       "      <td>-0.145885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Average Odds Difference</td>\n",
       "      <td>-0.097485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Equal Opportunity Difference</td>\n",
       "      <td>0.016573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Metric  Reweighing\n",
       "0             Balanced Accuracy    0.638525\n",
       "1             Accuracy Equality    0.228116\n",
       "2              Disparate Impact    0.830641\n",
       "3             Outcome Unfairess   -0.145885\n",
       "4       Average Odds Difference   -0.097485\n",
       "5  Equal Opportunity Difference    0.016573"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RW_results = {'Metric': ['Balanced Accuracy', 'Accuracy Equality', 'Disparate Impact', 'Outcome Unfairess', 'Average Odds Difference', 'Equal Opportunity Difference'],\n",
    "    'Reweighing': [RW_bal_acc, RW_acc_equal, RW_disp_imp, RW_out_unf, RW_avg_odds, RW_eq_odds]}\n",
    "RW_results_df = pd.DataFrame(RW_results)\n",
    "RW_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_json(filename = 'RW_results.json', df = RW_results_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
